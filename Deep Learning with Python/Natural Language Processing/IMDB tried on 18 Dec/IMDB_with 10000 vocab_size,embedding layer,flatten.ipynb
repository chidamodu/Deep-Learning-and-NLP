{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nkeras.__version__\n\nimport csv\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sentiment'].value_counts()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"positive    25000\nnegative    25000\nName: sentiment, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_labels and test_labels are lists of 0s and 1s, where 0 stands for \"negative\" and 1 stands for \"positive\"\n\n# df['sentiment']=np.where(df['sentiment']=='positive', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To get the maximum length of sentences in our dataset.\n\ndef max_len(df, c):\n    max_len=0\n    min_len=float(\"inf\")\n    for item in df[c]:\n        max_len=max(max_len, len(item))\n        min_len=min(min_len, len(item))\n    return max_len, min_len\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len, min_len=max_len(df, 'review')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len, min_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(13704+32)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['review'][3000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_special = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to remove stop words from column: review \nimport string\ndef remove_stop_words_punct(df, c):\n    sentences = []\n    output_sentences=[]\n    \n    for row in df[c]:\n        sentence = row\n        for word in stopwords_special:\n            token = \" \" + word + \" \"\n            sentence = sentence.replace(token, \" \")\n            sentence = sentence.replace(\"  \", \" \")\n        sentences.append(sentence)\n        \n    for s in sentences:\n        \n#         s.translate({ord(c): \" \" for c in ':\\'-,{!'})\n        element = s.replace('<br />','')\n        element = element.replace('\"','')\n        element = element.replace(')','')\n        element = element.replace('(','')\n        element = element.replace('{','')\n        element = element.replace('}','')\n        element = element.replace('\\'','')\n        element = element.replace('$','')\n        element = element.replace('-','')\n        element = element.replace('!','')\n        element = element.replace(':','')\n        element = element.replace(',','')\n        element = element.replace('.','')                          \n        output_sentences.append(' '.join( [w for w in element.split() if len(w)>1] ))\n\n    return output_sentences","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences=remove_stop_words_punct(df, 'review')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking good after the above transformation\n\nsentences[3000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=df['sentiment'].values.tolist()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting 80% of the dataset into a training set and 20% as validation set\n\ntraining_portion=0.80\n# testing portion=0.10\ntrain_size = int(len(sentences)*training_portion)\n\ntrain_sentences = sentences[:train_size]\ntrain_labels = labels[:train_size]\n\ntest_sentences = sentences[train_size:]\ntest_labels = labels[train_size:]\n\nprint(train_size)\nprint(len(train_sentences))\nprint(len(train_labels))\nprint(len(test_sentences))\nprint(len(test_labels))","execution_count":10,"outputs":[{"output_type":"stream","text":"40000\n40000\n40000\n10000\n10000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"40000*.20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_portion=0.20","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\noov_tok = \"<OOV>\"\nvocab_size=10000\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 6868 #average of max_len and min_len\ntrunc_type = 'post'\npadding_type = 'post'\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type,  padding=padding_type)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, truncating=trunc_type,  padding=padding_type)\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# y_train=np.array(train_labels)\n\n# y_test=np.array(test_labels)\n\nlabel_tokenizer = Tokenizer()\n\n\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_tokenizer","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<keras_preprocessing.text.Tokenizer at 0x7fa1b04d6350>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_tokenizer.fit_on_texts(labels)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_tokenizer.texts_to_sequences(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\ntest_label_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training_label_seq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(train_padded), len(training_label_seq), len(test_padded), len(test_label_seq)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partial_x_train = train_padded[:8000]\nx_validation = train_padded[8000:]\n\n\npartial_y_train = training_label_seq[:8000]\ny_validation = training_label_seq[8000:]\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try 1 hidden layer with 32 hidden units with relu activation\n\nfrom keras import models\nfrom keras.layers import Embedding\nfrom keras import layers\nfrom keras.layers import Flatten, Dense\nfrom keras.models import Sequential\nvocab_size=10000\nembedding_dim = 16\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(layers.Dense(1, activation='sigmoid'))# We add the classifier on top\n\n\n\n\n# model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#              tf.keras.layers.Dense(128, activation='relu'),\n#              tf.keras.layers.Dense(128, activation='relu'),\n#              tf.keras.layers.Dense(128, activation='relu'),\n#              tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n","execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Embedding' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-553928e75b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# We add the classifier on top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.compile(optimizer='rmsprop',\n#               loss='binary_crossentropy',\n#               metrics=['accuracy'])\n\nmodel.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A single hidden layer clearly is not sufficient to learn patterns from the training data and therefore performs poorly on the validation data also.\n#Remember that vicab_size=100000 words are included \n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    callbacks=[callback],\n                    validation_data=(x_validation, y_validation))\n\n\n\n# Received an InvalidArgumentError first: InvalidArgumentError:  Received a label value of 1 which is outside the valid range of [0, 1).\n# Range [0, 1) means every number between 0 and 1, excluding 1. So 1 is not a value in the range [0, 1).\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's plot Training Vs. Validation Loss\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's plot Training Vs. Validation Accuracy\n\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}