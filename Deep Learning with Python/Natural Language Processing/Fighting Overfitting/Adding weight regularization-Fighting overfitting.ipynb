{"cells":[{"metadata":{},"cell_type":"markdown","source":"Adding weight regularization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import imdb\nimport numpy as np\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\ndef vectorize_sequences(sequences, dimension=10000):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n    return results\n\n# Our vectorized training data\nx_train = vectorize_sequences(train_data)\n# Our vectorized test data\nx_test = vectorize_sequences(test_data)\n# Our vectorized labels\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","execution_count":1,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_model = models.Sequential()\noriginal_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\noriginal_model.add(layers.Dense(16, activation='relu'))\noriginal_model.add(layers.Dense(1, activation='sigmoid'))\n\noriginal_model.compile(optimizer='rmsprop',\n                       loss='binary_crossentropy',\n                       metrics=['acc'])","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_hist = original_model.fit(x_train, y_train,\n                                   epochs=20,\n                                   batch_size=512,\n                                   validation_data=(x_test, y_test))","execution_count":8,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n49/49 [==============================] - 2s 47ms/step - loss: 0.4904 - acc: 0.8179 - val_loss: 0.3703 - val_acc: 0.8639\nEpoch 2/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.2764 - acc: 0.9063 - val_loss: 0.3051 - val_acc: 0.8778\nEpoch 3/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.2096 - acc: 0.9262 - val_loss: 0.2835 - val_acc: 0.8871\nEpoch 4/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.1713 - acc: 0.9397 - val_loss: 0.2862 - val_acc: 0.8860\nEpoch 5/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.1458 - acc: 0.9504 - val_loss: 0.3020 - val_acc: 0.8821\nEpoch 6/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.1247 - acc: 0.9573 - val_loss: 0.3331 - val_acc: 0.8754\nEpoch 7/20\n49/49 [==============================] - 1s 21ms/step - loss: 0.1050 - acc: 0.9643 - val_loss: 0.3560 - val_acc: 0.8724\nEpoch 8/20\n49/49 [==============================] - 1s 27ms/step - loss: 0.0908 - acc: 0.9704 - val_loss: 0.3922 - val_acc: 0.8676\nEpoch 9/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.0743 - acc: 0.9770 - val_loss: 0.4098 - val_acc: 0.8666\nEpoch 10/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.0605 - acc: 0.9831 - val_loss: 0.4368 - val_acc: 0.8644\nEpoch 11/20\n49/49 [==============================] - 1s 21ms/step - loss: 0.0501 - acc: 0.9870 - val_loss: 0.4644 - val_acc: 0.8623\nEpoch 12/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.0401 - acc: 0.9900 - val_loss: 0.5055 - val_acc: 0.8600\nEpoch 13/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.0313 - acc: 0.9921 - val_loss: 0.5442 - val_acc: 0.8587\nEpoch 14/20\n49/49 [==============================] - 1s 24ms/step - loss: 0.0242 - acc: 0.9945 - val_loss: 0.6220 - val_acc: 0.8518\nEpoch 15/20\n49/49 [==============================] - 1s 27ms/step - loss: 0.0197 - acc: 0.9960 - val_loss: 0.6253 - val_acc: 0.8552\nEpoch 16/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.0161 - acc: 0.9964 - val_loss: 0.6548 - val_acc: 0.8552\nEpoch 17/20\n49/49 [==============================] - 1s 26ms/step - loss: 0.0112 - acc: 0.9980 - val_loss: 0.6960 - val_acc: 0.8552\nEpoch 18/20\n49/49 [==============================] - 1s 25ms/step - loss: 0.0088 - acc: 0.9983 - val_loss: 0.7471 - val_acc: 0.8525\nEpoch 19/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.0076 - acc: 0.9980 - val_loss: 0.7776 - val_acc: 0.8530\nEpoch 20/20\n49/49 [==============================] - 1s 21ms/step - loss: 0.0069 - acc: 0.9985 - val_loss: 0.8176 - val_acc: 0.8529\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nfrom keras import models\nfrom keras import layers\n\nl2_model = models.Sequential()\nl2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n                          activation='relu', input_shape=(10000,)))\nl2_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n                          activation='relu'))\nl2_model.add(layers.Dense(1, activation='sigmoid'))","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"l2(0.001) means that every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time."},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_model.compile(optimizer='rmsprop',\n                 loss='binary_crossentropy',\n                 metrics=['acc'])","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_model_hist = l2_model.fit(x_train, y_train,\n                             epochs=20,\n                             batch_size=512,\n                             validation_data=(x_test, y_test))","execution_count":5,"outputs":[{"output_type":"stream","text":"Epoch 1/20\n49/49 [==============================] - 3s 56ms/step - loss: 0.5686 - acc: 0.7697 - val_loss: 0.4366 - val_acc: 0.8726\nEpoch 2/20\n49/49 [==============================] - 1s 24ms/step - loss: 0.3483 - acc: 0.8991 - val_loss: 0.3428 - val_acc: 0.8867\nEpoch 3/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2797 - acc: 0.9176 - val_loss: 0.3299 - val_acc: 0.8878\nEpoch 4/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2523 - acc: 0.9286 - val_loss: 0.3341 - val_acc: 0.8861\nEpoch 5/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2373 - acc: 0.9330 - val_loss: 0.3436 - val_acc: 0.8811\nEpoch 6/20\n49/49 [==============================] - 1s 29ms/step - loss: 0.2267 - acc: 0.9382 - val_loss: 0.3761 - val_acc: 0.8733\nEpoch 7/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2218 - acc: 0.9397 - val_loss: 0.3539 - val_acc: 0.8800\nEpoch 8/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.2148 - acc: 0.9430 - val_loss: 0.3637 - val_acc: 0.8791\nEpoch 9/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2090 - acc: 0.9445 - val_loss: 0.3753 - val_acc: 0.8731\nEpoch 10/20\n49/49 [==============================] - 1s 24ms/step - loss: 0.2063 - acc: 0.9464 - val_loss: 0.3717 - val_acc: 0.8779\nEpoch 11/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.2034 - acc: 0.9460 - val_loss: 0.3882 - val_acc: 0.8715\nEpoch 12/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.1960 - acc: 0.9501 - val_loss: 0.4091 - val_acc: 0.8703\nEpoch 13/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.1963 - acc: 0.9498 - val_loss: 0.4553 - val_acc: 0.8515\nEpoch 14/20\n49/49 [==============================] - 1s 22ms/step - loss: 0.1910 - acc: 0.9524 - val_loss: 0.3981 - val_acc: 0.8733\nEpoch 15/20\n49/49 [==============================] - 2s 31ms/step - loss: 0.1880 - acc: 0.9526 - val_loss: 0.4295 - val_acc: 0.8620\nEpoch 16/20\n49/49 [==============================] - 1s 29ms/step - loss: 0.1845 - acc: 0.9557 - val_loss: 0.4230 - val_acc: 0.8662\nEpoch 17/20\n49/49 [==============================] - 1s 23ms/step - loss: 0.1842 - acc: 0.9555 - val_loss: 0.4174 - val_acc: 0.8673\nEpoch 18/20\n49/49 [==============================] - 1s 27ms/step - loss: 0.1792 - acc: 0.9570 - val_loss: 0.4312 - val_acc: 0.8645\nEpoch 19/20\n49/49 [==============================] - 1s 24ms/step - loss: 0.1787 - acc: 0.9575 - val_loss: 0.4236 - val_acc: 0.8677\nEpoch 20/20\n49/49 [==============================] - 1s 24ms/step - loss: 0.1763 - acc: 0.9590 - val_loss: 0.4413 - val_acc: 0.8670\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noriginal_val_loss = original_hist.history['val_loss']\nl2_model_val_loss = l2_model_hist.history['val_loss']","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(epochs, original_val_loss, 'b+', label='Original model')\nplt.plot(epochs, l2_model_val_loss, 'bo', label='L2-regularized model')\nplt.xlabel('Epochs')\nplt.ylabel('Validation loss')\nplt.legend()\n\nplt.show()\n","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), even though both models have the same number of parameters.\n\n"},{"metadata":{},"cell_type":"markdown","source":"As alternatives to L2 regularization, you could use one of the following Keras weight regularizers:\n\nfrom keras import regularizers\n\nL1 regularization\nregularizers.l1(0.001)\n\nL1 and L2 regularization at the same time\nregularizers.l1_l2(l1=0.001, l2=0.001)"},{"metadata":{},"cell_type":"markdown","source":"Adding dropout"},{"metadata":{},"cell_type":"markdown","source":"In Keras you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before it, e.g.:\n    \nmodel.add(layers.Dropout(0.5))"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's add two Dropout layers in our IMDB network to see how well they do at reducing overfitting:\n\ndpt_model = models.Sequential()\ndpt_model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(16, activation='relu'))\ndpt_model.add(layers.Dropout(0.5))\ndpt_model.add(layers.Dense(1, activation='sigmoid'))\n\ndpt_model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpt_model_hist = dpt_model.fit(x_train, y_train,\n                               epochs=20,\n                               batch_size=512,\n                               validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpt_model_val_loss = dpt_model_hist.history['val_loss']\n\nplt.plot(epochs, original_val_loss, 'b+', label='Original model')\nplt.plot(epochs, dpt_model_val_loss, 'bo', label='Dropout-regularized model')\nplt.xlabel('Epochs')\nplt.ylabel('Validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, a clear improvement over the reference network."},{"metadata":{},"cell_type":"markdown","source":"To recap: here the most common ways to prevent overfitting in neural networks:\n\nGetting more training data.\nReducing the capacity of the network.\nAdding weight regularization.\nAdding dropout."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}