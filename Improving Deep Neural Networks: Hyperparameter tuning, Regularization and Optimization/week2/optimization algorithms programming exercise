The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples 
you use to perform one update step.
You have to tune a learning rate hyperparameter  Î±.
With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent
(particularly when the training set is large).
