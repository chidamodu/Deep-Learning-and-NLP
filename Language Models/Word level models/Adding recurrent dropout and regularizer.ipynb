{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n### YOUR CODE HERE\nfrom tensorflow.keras import regularizers\n###\nimport tensorflow.keras.utils as ku \nimport numpy as np\nimport keras\nfrom keras import layers\nfrom keras.regularizers import l2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n    -O /tmp/sonnets.txt\ndata = open('/tmp/sonnets.txt').read()\n\ncorpus = data.lower().split(\"\\n\")\n\n\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1\n\n# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n\tfor i in range(1, len(token_list)):\n\t\tn_gram_sequence = token_list[:i+1]\n\t\tinput_sequences.append(n_gram_sequence)","execution_count":2,"outputs":[{"output_type":"stream","text":"--2021-02-10 20:33:03--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.128, 173.194.217.128, 172.217.204.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93578 (91K) [text/plain]\nSaving to: ‘/tmp/sonnets.txt’\n\n/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n\n2021-02-10 20:33:03 (78.6 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n\nlabel = ku.to_categorical(label, num_classes=total_words)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = keras.models.Sequential()\nmodel.add(Embedding(total_words, 300, input_length=max_sequence_len-1))\nmodel.add(layers.Bidirectional(layers.LSTM(512, recurrent_dropout=0.3, recurrent_regularizer=l2(0.01),return_sequences=True)))\nmodel.add(layers.LSTM(256))\nmodel.add(layers.Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":11,"outputs":[{"output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 10, 300)           963300    \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 10, 1024)          3330048   \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 256)               1311744   \n_________________________________________________________________\ndense_3 (Dense)              (None, 3211)              825227    \n=================================================================\nTotal params: 6,430,319\nTrainable params: 6,430,319\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"address=\"./kaggle/working/weights-improvement.hdf5\"\nstop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, \n                             patience = 5, verbose = 1, mode = 'auto')\nsave = ModelCheckpoint(address, monitor = 'val_accuracy', \n                               verbose = 0, save_best_only = True)\ncallbacks = [stop, save]\n\nhistory = model.fit(predictors, label, validation_split=0.20, epochs=50, verbose=1, callbacks = callbacks)","execution_count":12,"outputs":[{"output_type":"stream","text":"Epoch 1/50\n387/387 [==============================] - 35s 90ms/step - loss: 7.2038 - accuracy: 0.0267 - val_loss: 6.7779 - val_accuracy: 0.0265\nEpoch 2/50\n387/387 [==============================] - 33s 87ms/step - loss: 6.3385 - accuracy: 0.0393 - val_loss: 6.8523 - val_accuracy: 0.0252\nEpoch 3/50\n387/387 [==============================] - 34s 87ms/step - loss: 6.0843 - accuracy: 0.0483 - val_loss: 6.9549 - val_accuracy: 0.0375\nEpoch 4/50\n387/387 [==============================] - 33s 86ms/step - loss: 5.7916 - accuracy: 0.0580 - val_loss: 7.0830 - val_accuracy: 0.0459\nEpoch 5/50\n387/387 [==============================] - 33s 86ms/step - loss: 5.4554 - accuracy: 0.0716 - val_loss: 7.2374 - val_accuracy: 0.0430\nEpoch 6/50\n387/387 [==============================] - 34s 87ms/step - loss: 5.0898 - accuracy: 0.0917 - val_loss: 7.3632 - val_accuracy: 0.0514\nEpoch 7/50\n387/387 [==============================] - 33s 85ms/step - loss: 4.6955 - accuracy: 0.1189 - val_loss: 7.5587 - val_accuracy: 0.0521\nEpoch 8/50\n387/387 [==============================] - 33s 87ms/step - loss: 4.2888 - accuracy: 0.1641 - val_loss: 7.7430 - val_accuracy: 0.0556\nEpoch 9/50\n387/387 [==============================] - 34s 87ms/step - loss: 3.8894 - accuracy: 0.2267 - val_loss: 7.8783 - val_accuracy: 0.0472\nEpoch 10/50\n387/387 [==============================] - 34s 87ms/step - loss: 3.4938 - accuracy: 0.2979 - val_loss: 8.0761 - val_accuracy: 0.0475\nEpoch 11/50\n387/387 [==============================] - 34s 87ms/step - loss: 3.1101 - accuracy: 0.3771 - val_loss: 8.2290 - val_accuracy: 0.0456\nEpoch 12/50\n387/387 [==============================] - 33s 85ms/step - loss: 2.7640 - accuracy: 0.4469 - val_loss: 8.4642 - val_accuracy: 0.0446\nEpoch 13/50\n387/387 [==============================] - 34s 87ms/step - loss: 2.4510 - accuracy: 0.5211 - val_loss: 8.6060 - val_accuracy: 0.0411\nEpoch 00013: early stopping\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Adding recurrent_dropout=0.3 and recurrent_regularizer=l2(0.01) did not improve the model's performance. It could have been inadvertently damaging the performance by randomly dropping the recurrent state, i.e., the output from the previous layers."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}