{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n### YOUR CODE HERE\nfrom tensorflow.keras import regularizers\n###\nimport tensorflow.keras.utils as ku \nimport numpy as np\nimport keras\nfrom keras import layers\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n    -O /tmp/sonnets.txt\ndata = open('/tmp/sonnets.txt').read()\n\ncorpus = data.lower().split(\"\\n\")\n\n\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1\n\n# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n\tfor i in range(1, len(token_list)):\n\t\tn_gram_sequence = token_list[:i+1]\n\t\tinput_sequences.append(n_gram_sequence)","execution_count":2,"outputs":[{"output_type":"stream","text":"--2021-02-04 07:47:37--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.210.128, 142.250.98.128, 172.217.193.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.210.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 93578 (91K) [text/plain]\nSaving to: ‘/tmp/sonnets.txt’\n\n/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n\n2021-02-04 07:47:37 (86.8 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_words","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"3211"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corpus)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"2159"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[:10]","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"['from fairest creatures we desire increase,',\n \"that thereby beauty's rose might never die,\",\n 'but as the riper should by time decease,',\n 'his tender heir might bear his memory:',\n 'but thou, contracted to thine own bright eyes,',\n \"feed'st thy light'st flame with self-substantial fuel,\",\n 'making a famine where abundance lies,',\n 'thyself thy foe, to thy sweet self too cruel.',\n \"thou that art now the world's fresh ornament\",\n 'and only herald to the gaudy spring,']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n\nlabel = ku.to_categorical(label, num_classes=total_words)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictors.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(15462, 10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_sequence_len demonstrates the max number of words in sentence\nmax_sequence_len-1","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"10"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = keras.models.Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(layers.Dense(total_words, activation='relu'))\nmodel.add(layers.LSTM(128, return_sequences=True))\nmodel.add(layers.LSTM(128, return_sequences=True))\nmodel.add(layers.LSTM(128))\nmodel.add(layers.Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics='accuracy')\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 10, 100)           321100    \n_________________________________________________________________\ndense (Dense)                (None, 10, 3211)          324311    \n_________________________________________________________________\nlstm (LSTM)                  (None, 10, 128)           1710080   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 10, 128)           131584    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 128)               131584    \n_________________________________________________________________\ndense_1 (Dense)              (None, 3211)              414219    \n=================================================================\nTotal params: 3,032,878\nTrainable params: 3,032,878\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# where does the 484 in each epoch come from?\nhistory = model.fit(predictors, label, epochs=100, verbose=1)","execution_count":9,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n484/484 [==============================] - 5s 11ms/step - loss: 6.8831 - accuracy: 0.0204\nEpoch 2/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.5072 - accuracy: 0.0237\nEpoch 3/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.4003 - accuracy: 0.0330\nEpoch 4/100\n484/484 [==============================] - 6s 11ms/step - loss: 6.3095 - accuracy: 0.0360\nEpoch 5/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.2137 - accuracy: 0.0395\nEpoch 6/100\n484/484 [==============================] - 5s 11ms/step - loss: 6.1019 - accuracy: 0.0408\nEpoch 7/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.9797 - accuracy: 0.0480\nEpoch 8/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.8558 - accuracy: 0.0487\nEpoch 9/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.7305 - accuracy: 0.0548\nEpoch 10/100\n484/484 [==============================] - 6s 12ms/step - loss: 5.6038 - accuracy: 0.0593\nEpoch 11/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.4894 - accuracy: 0.0616\nEpoch 12/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.3680 - accuracy: 0.0659\nEpoch 13/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.2424 - accuracy: 0.0687\nEpoch 14/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.1140 - accuracy: 0.0753\nEpoch 15/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.9825 - accuracy: 0.0812\nEpoch 16/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.8499 - accuracy: 0.0869\nEpoch 17/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.7197 - accuracy: 0.0924\nEpoch 18/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.5888 - accuracy: 0.0972\nEpoch 19/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.4610 - accuracy: 0.1088\nEpoch 20/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.3372 - accuracy: 0.1224\nEpoch 21/100\n484/484 [==============================] - 6s 12ms/step - loss: 4.2196 - accuracy: 0.1418\nEpoch 22/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.1060 - accuracy: 0.1586\nEpoch 23/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.9929 - accuracy: 0.1781\nEpoch 24/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.8823 - accuracy: 0.1977\nEpoch 25/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.7790 - accuracy: 0.2095\nEpoch 26/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.6798 - accuracy: 0.2312\nEpoch 27/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.5830 - accuracy: 0.2480\nEpoch 28/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.4922 - accuracy: 0.2624\nEpoch 29/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.4011 - accuracy: 0.2767\nEpoch 30/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.3145 - accuracy: 0.2898\nEpoch 31/100\n484/484 [==============================] - 6s 12ms/step - loss: 3.2388 - accuracy: 0.3068\nEpoch 32/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.1620 - accuracy: 0.3201\nEpoch 33/100\n484/484 [==============================] - 5s 11ms/step - loss: 3.0866 - accuracy: 0.3342\nEpoch 34/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.0150 - accuracy: 0.3432\nEpoch 35/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.9468 - accuracy: 0.3608\nEpoch 36/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.8737 - accuracy: 0.3758\nEpoch 37/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.8085 - accuracy: 0.3855\nEpoch 38/100\n484/484 [==============================] - 6s 12ms/step - loss: 2.7423 - accuracy: 0.4022\nEpoch 39/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.6744 - accuracy: 0.4141\nEpoch 40/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.6169 - accuracy: 0.4271\nEpoch 41/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.5605 - accuracy: 0.4380\nEpoch 42/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.4974 - accuracy: 0.4529\nEpoch 43/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.4380 - accuracy: 0.4658\nEpoch 44/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.3860 - accuracy: 0.4725\nEpoch 45/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.3295 - accuracy: 0.4892\nEpoch 46/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.2696 - accuracy: 0.5006\nEpoch 47/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.2193 - accuracy: 0.5103\nEpoch 48/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.1731 - accuracy: 0.5221\nEpoch 49/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.1189 - accuracy: 0.5345\nEpoch 50/100\n484/484 [==============================] - 5s 11ms/step - loss: 2.0671 - accuracy: 0.5455\nEpoch 51/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.0169 - accuracy: 0.5584\nEpoch 52/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.9676 - accuracy: 0.5667\nEpoch 53/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.9262 - accuracy: 0.5801\nEpoch 54/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.8817 - accuracy: 0.5865\nEpoch 55/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.8301 - accuracy: 0.5994\nEpoch 56/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7886 - accuracy: 0.6103\nEpoch 57/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7460 - accuracy: 0.6183\nEpoch 58/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.6992 - accuracy: 0.6303\nEpoch 59/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6678 - accuracy: 0.6359\nEpoch 60/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6211 - accuracy: 0.6495\nEpoch 61/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.5828 - accuracy: 0.6579\nEpoch 62/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3767 - accuracy: 0.7062\nEpoch 68/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.3378 - accuracy: 0.7166\nEpoch 69/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3117 - accuracy: 0.7209\nEpoch 70/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2945 - accuracy: 0.7236\nEpoch 71/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2530 - accuracy: 0.7328\nEpoch 72/100\n484/484 [==============================] - 6s 11ms/step - loss: 1.2172 - accuracy: 0.7403\nEpoch 73/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1970 - accuracy: 0.7442\nEpoch 74/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1714 - accuracy: 0.7521\nEpoch 75/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1521 - accuracy: 0.7569\nEpoch 76/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1197 - accuracy: 0.7632\nEpoch 77/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.1008 - accuracy: 0.7670\nEpoch 78/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.0727 - accuracy: 0.7752\nEpoch 79/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.0519 - accuracy: 0.7788\nEpoch 80/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.0306 - accuracy: 0.7853\nEpoch 81/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.0155 - accuracy: 0.7839\nEpoch 82/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.9950 - accuracy: 0.7892\nEpoch 83/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.9700 - accuracy: 0.7956\nEpoch 84/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.9470 - accuracy: 0.7983\nEpoch 85/100\n","name":"stdout"},{"output_type":"stream","text":"484/484 [==============================] - 5s 10ms/step - loss: 0.9324 - accuracy: 0.8018\nEpoch 86/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.9106 - accuracy: 0.8082\nEpoch 87/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.8974 - accuracy: 0.8066\nEpoch 88/100\n484/484 [==============================] - 6s 11ms/step - loss: 0.8930 - accuracy: 0.8090\nEpoch 89/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.8798 - accuracy: 0.8113\nEpoch 90/100\n484/484 [==============================] - 5s 11ms/step - loss: 0.8614 - accuracy: 0.8149\nEpoch 91/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.8362 - accuracy: 0.8197\nEpoch 92/100\n484/484 [==============================] - 5s 11ms/step - loss: 0.8232 - accuracy: 0.8205\nEpoch 93/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.8176 - accuracy: 0.8214\nEpoch 94/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.8002 - accuracy: 0.8236\nEpoch 95/100\n484/484 [==============================] - 5s 11ms/step - loss: 0.7845 - accuracy: 0.8256\nEpoch 96/100\n484/484 [==============================] - 5s 11ms/step - loss: 0.7768 - accuracy: 0.8277\nEpoch 97/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.7728 - accuracy: 0.8282\nEpoch 98/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.7647 - accuracy: 0.8293\nEpoch 99/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.7620 - accuracy: 0.8299\nEpoch 100/100\n484/484 [==============================] - 5s 10ms/step - loss: 0.7388 - accuracy: 0.8322\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#couldn't get it to work\n\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#couldn't get it to work\n#outcome of the seed text without - with learning rate chosen for Adam optimizer \nseed_text = \"Hell is empty\"\nnext_words = 100\n  \nfor _ in range(next_words):\n\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n\tpredicted = model.predict_classes(token_list, verbose=0)\n\toutput_word = \"\"\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == predicted:\n\t\t\toutput_word = word\n\t\t\tbreak\n\tseed_text += \" \" + output_word\nprint(seed_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trained the model for 100 epochs and that made a difference.\n\nThe accuracy jumped from 50% to 83% Also the loss went down from 2% to 0.7% Whoa that's great. Now what? Maybe we can optimize the model building and get to a computationally less intensive one?"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}